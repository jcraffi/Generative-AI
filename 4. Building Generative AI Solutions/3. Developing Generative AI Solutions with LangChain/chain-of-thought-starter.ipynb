{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can solve a simple math problem with gpt-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add prereq code to set open API key\n",
    "\n",
    "import os\n",
    "OPENAI_API_KEY=os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Got unknown type \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m instruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124mAndy harvests all the tomatoes from 18 plants that have 7 tomatoes each. If he dries half the\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124mtomatoes and turns a third of the remainder into marinara sauce, how many tomatoes are left?\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Use the new API to generate a response\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Answer: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    610\u001b[0m     messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[BaseMessage]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    619\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    620\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pass a sequence of prompts to the model and return model generations.\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m    This method should make use of batched calls for models that expose a batched\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m    API.\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \n\u001b[1;32m    625\u001b[0m \u001b[38;5;124;03m    Use this method when you want to:\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;124;03m        1. take advantage of batched calls,\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m        2. need more output from the model than just the top generated value,\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m        3. are building chains that are agnostic to the underlying language model\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m            type (e.g., pure text completion models vs chat models).\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \n\u001b[1;32m    631\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;124;03m        messages: List of list of messages.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03m        stop: Stop words to use when generating. Model output is cut off at the\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m            first occurrence of any of these substrings.\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m        callbacks: Callbacks to pass through. Used for executing additional\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m            functionality, such as logging or streaming, throughout generation.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m        **kwargs: Arbitrary additional keyword arguments. These are usually passed\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m            to the model provider API call.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m        An LLMResult, which contains a list of candidate Generations for each input\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03m            prompt and additional model provider-specific output.\u001b[39;00m\n\u001b[0;32m--> 643\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    644\u001b[0m     structured_output_format \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstructured_output_format\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m structured_output_format:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    610\u001b[0m     messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[BaseMessage]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    619\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    620\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pass a sequence of prompts to the model and return model generations.\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m    This method should make use of batched calls for models that expose a batched\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m    API.\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \n\u001b[1;32m    625\u001b[0m \u001b[38;5;124;03m    Use this method when you want to:\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;124;03m        1. take advantage of batched calls,\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m        2. need more output from the model than just the top generated value,\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m        3. are building chains that are agnostic to the underlying language model\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m            type (e.g., pure text completion models vs chat models).\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \n\u001b[1;32m    631\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;124;03m        messages: List of list of messages.\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m \u001b[38;5;124;03m        stop: Stop words to use when generating. Model output is cut off at the\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m            first occurrence of any of these substrings.\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m        callbacks: Callbacks to pass through. Used for executing additional\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m            functionality, such as logging or streaming, throughout generation.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;124;03m        **kwargs: Arbitrary additional keyword arguments. These are usually passed\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m            to the model provider API call.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m        An LLMResult, which contains a list of candidate Generations for each input\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03m            prompt and additional model provider-specific output.\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    644\u001b[0m     structured_output_format \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstructured_output_format\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m structured_output_format:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_community/chat_models/openai.py:470\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    466\u001b[0m     stream_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[1;32m    467\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    468\u001b[0m     )\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[0;32m--> 470\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_message_dicts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    475\u001b[0m }\n\u001b[1;32m    476\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_with_retry(\n\u001b[1;32m    477\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    478\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_community/chat_models/openai.py:489\u001b[0m, in \u001b[0;36mChatOpenAI._create_message_dicts\u001b[0;34m(self, messages, stop)\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`stop` found in both the input and default params.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    488\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n\u001b[0;32m--> 489\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m [convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message_dicts, params\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_community/chat_models/openai.py:489\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`stop` found in both the input and default params.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    488\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n\u001b[0;32m--> 489\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m [\u001b[43mconvert_message_to_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message_dicts, params\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_community/adapters/openai.py:160\u001b[0m, in \u001b[0;36mconvert_message_to_dict\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    154\u001b[0m     message_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_call_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mtool_call_id,\n\u001b[1;32m    158\u001b[0m     }\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unknown type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39madditional_kwargs:\n\u001b[1;32m    162\u001b[0m     message_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39madditional_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: Got unknown type \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "temperature = 0.0\n",
    "llm = ChatOpenAI(model_name=model_name, temperature=temperature, max_tokens = 500)\n",
    "\n",
    "# Note: example taken from\n",
    "# https://browse.arxiv.org/pdf/2303.12712.pdf\n",
    "\n",
    "instruction = \"\"\"\n",
    "Andy harvests all the tomatoes from 18 plants that have 7 tomatoes each. If he dries half the\n",
    "tomatoes and turns a third of the remainder into marinara sauce, how many tomatoes are left?\n",
    "\"\"\"\n",
    "\n",
    "# Use the new API to generate a response\n",
    "response = llm.generate(instruction)\n",
    "print(\"Original Answer: \")\n",
    "print(response.generations[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run this, we see an answer:\n",
    "Original Answer: \n",
    "Andy harvests a total of 18 plants * 7 tomatoes/plant = <<18*7=126>>126 tomatoes.\n",
    "He dries half of the tomatoes, so he has 126 tomatoes / 2 = <<126/2=63>>63 tomatoes left.\n",
    "He turns a third of the remaining tomatoes into marinara sauce, so he has 63 tomatoes * 1/3 = <<63*1/3=21>>21 tomatoes left. Answer: \\boxed{21}.\n",
    "\n",
    "In reality, Andy will have 63 - 21 = 42 tomatoes left\n",
    "\n",
    "Let's see how we can do this with Chain Of Thought Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's setup a few examples of how we want LLM to think about the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question1 = \"\"\"\n",
    "Karen harvests all the pears from 20 trees that have 10 pears each. She  throws a third of them away as they are rotten,\n",
    "and turns a quarter of the remaining ones into jam. How many are left?\n",
    "\"\"\"\n",
    "answer1 = \"\"\"\n",
    "    First, let's calculate how many pears Gloria harvests: it's 20 * 10 = 200. \n",
    "    Then, let's calculate how many are rotten: 200 * 1/3 = 66.\n",
    "    Thus, we know how many are left after she throws a third of them away: 200 - 66 = 134.\n",
    "    1/4 of the remaining ones are turned into jam, or 134 * 1/4 = 33. Therefore, Karen is left with 134 - 33, or 101 pears\n",
    "\"\"\"\n",
    "question2 = \"\"\"\n",
    "Sergei harvests all the strawberries from 50 plants that have 8 stawberries each. He freezes a quarter of them,\n",
    "and turns half of the remaining ones into jam. How many are left?\n",
    "\"\"\"\n",
    "answer2 = \"\"\"\n",
    "    First, let's calculate how many strawberries Sergei harvests: it's 50 * 8 = 400. \n",
    "    Then, let's calculate how many are frozen: 400 * 1/4 = 100.\n",
    "    Thus, we know how many are left after he freezes 100 of them: 400 - 100 = 300.\n",
    "    half of the remaining ones are turned into jam, or 300 * 1/2 = 150. Therefore, Sergei is left with 300 - 150, or 150 pears\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's setup a prompt template for one question/answer pair, and put our examples into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"{question}\\n{answer}\")\n",
    "# TODO: setup an array of question / answer examples\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": question1,\n",
    "        \"answer\": answer1\n",
    "    },\n",
    "    {\n",
    "        \"question\": question2,\n",
    "        \"answer\": answer2\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's setup the FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: setup chain of thought prompt template based on few shot prompt template\n",
    "cot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt = example_prompt,\n",
    "    suffix=\"Use these questions and answers to give correct response to the problem below: {input}\",\n",
    "    input_variables = ['input']\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use the same instruction as before and see if we get a different answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chain of Thought Prompt ===\n",
      "\n",
      "Karen harvests all the pears from 20 trees that have 10 pears each. She  throws a third of them away as they are rotten,\n",
      "and turns a quarter of the remaining ones into jam. How many are left?\n",
      "\n",
      "\n",
      "    First, let's calculate how many pears Gloria harvests: it's 20 * 10 = 200. \n",
      "    Then, let's calculate how many are rotten: 200 * 1/3 = 66.\n",
      "    Thus, we know how many are left after she throws a third of them away: 200 - 66 = 134.\n",
      "    1/4 of the remaining ones are turned into jam, or 134 * 1/4 = 33. Therefore, Karen is left with 134 - 33, or 101 pears\n",
      "\n",
      "\n",
      "\n",
      "Sergei harvests all the strawberries from 50 plants that have 8 stawberries each. He freezes a quarter of them,\n",
      "and turns half of the remaining ones into jam. How many are left?\n",
      "\n",
      "\n",
      "    First, let's calculate how many strawberries Sergei harvests: it's 50 * 8 = 400. \n",
      "    Then, let's calculate how many are frozen: 400 * 1/4 = 100.\n",
      "    Thus, we know how many are left after he freezes 100 of them: 400 - 100 = 300.\n",
      "    half of the remaining ones are turned into jam, or 300 * 1/2 = 150. Therefore, Sergei is left with 300 - 150, or 150 pears\n",
      "\n",
      "\n",
      "Use these questions and answers to give correct response to the problem below: \n",
      "Andy harvests all the tomatoes from 18 plants that have 7 tomatoes each. If he dries half the\n",
      "tomatoes and turns a third of the remainder into marinara sauce, how many tomatoes are left?\n",
      "\n",
      "=== Chain of Thought Answer ===\n",
      "First, let's calculate how many tomatoes Andy harvests: it's 18 * 7 = 126.\n",
      "Then, let's calculate how many are dried: 126 * 1/2 = 63.\n",
      "Thus, we know how many are left after he dries 63 of them: 126 - 63 = 63.\n",
      "A third of the remaining ones are turned into marinara sauce, or 63 * 1/3 = 21. Therefore, Andy is left with 63 - 21, or 42 tomatoes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cot_text = cot_prompt.format(input=instruction)\n",
    "print(\"=== Chain of Thought Prompt ===\")\n",
    "print(cot_text)\n",
    "print(\"=== Chain of Thought Answer ===\")\n",
    "print(llm(cot_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
